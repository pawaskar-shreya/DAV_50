{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu7eGWd3MyALcV/lkRN3YA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawaskar-shreya/DAV_50/blob/main/DAV_Exp7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment - 7: Perform the steps involved in Text Analytics in Python**"
      ],
      "metadata": {
        "id": "yIH0srIBINUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the most used text analytics libraries in Python are NLTK, spaCy, TextBlob, Gensim, Transformers"
      ],
      "metadata": {
        "id": "yWsTBGEYIazL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom dataset as a string\n",
        "custom_dataset = \"What is a sentence? A sentence is a group of words that makes complete sense.\"\n",
        "\n",
        "# Print the custom dataset\n",
        "print(\"Custom Dataset:\")\n",
        "print(custom_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3jJU7EJMIvr",
        "outputId": "f6b3f371-4a2a-4242-ca11-73bce5d274e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Dataset:\n",
            "What is a sentence? A sentence is a group of words that makes complete sense.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization (Sentence & Word)\n",
        "sentences = sent_tokenize(custom_dataset)\n",
        "words = word_tokenize(custom_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(\"Tokenization (Sentences):\", sentences)\n",
        "print(\"\\nTokenization (Words):\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHup8Qm5NOtz",
        "outputId": "a09625c0-f3cc-4895-cdbb-26dbea1a1a79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization (Sentences): ['What is a sentence?', 'A sentence is a group of words that makes complete sense.']\n",
            "\n",
            "Tokenization (Words): ['What', 'is', 'a', 'sentence', '?', 'A', 'sentence', 'is', 'a', 'group', 'of', 'words', 'that', 'makes', 'complete', 'sense', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print the results\n",
        "print(\"Frequency Distribution:\")\n",
        "print(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYOLcKX6NX10",
        "outputId": "9304351b-975b-415d-9ba7-dd73b9c955b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution:\n",
            "<FreqDist with 14 samples and 17 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if (word.isalpha() and word.lower() not in stop_words)]\n",
        "\n",
        "# Print the results\n",
        "print(\"Filtered Words (without stopwords and punctuations):\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JKM4xfVO3oO",
        "outputId": "2094e716-30c9-4ec5-abdd-551f98412eab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words (without stopwords and punctuations):\n",
            "['sentence', 'sentence', 'group', 'words', 'makes', 'complete', 'sense']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in filtered_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Print the results\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9DMU-zfPIlF",
        "outputId": "41323ec2-af0c-4379-9a12-250ccc5d9c5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words:\n",
            "['sentenc', 'sentenc', 'group', 'word', 'make', 'complet', 'sens']\n",
            "\n",
            "Lemmatized Words:\n",
            "['sentence', 'sentence', 'group', 'word', 'make', 'complete', 'sense']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Part of Speech tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the results\n",
        "print(\"Part of Speech Tags:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F9BPE3EPfBE",
        "outputId": "9e65c292-c536-4113-d982-755e133fdf8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech Tags:\n",
            "[('What', 'WP'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('?', '.'), ('A', 'DT'), ('sentence', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('group', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('that', 'WDT'), ('makes', 'VBZ'), ('complete', 'JJ'), ('sense', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Named Entity Recognition\n",
        "ner_tags = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# Print the results\n",
        "print(\"Named Entity Recognition Tags:\")\n",
        "print(ner_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q-cWY48RgI9",
        "outputId": "68275dcb-547d-4488-afac-5dc79d829a3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recognition Tags:\n",
            "(S\n",
            "  What/WP\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  sentence/NN\n",
            "  ?/.\n",
            "  A/DT\n",
            "  sentence/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  group/NN\n",
            "  of/IN\n",
            "  words/NNS\n",
            "  that/WDT\n",
            "  makes/VBZ\n",
            "  complete/JJ\n",
            "  sense/NN\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the most used text analytics libraries in R are tm, quanteda, tidytext, text, textTinyR\n"
      ],
      "metadata": {
        "id": "jSk1S6gtI627"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom dataset as a string\n",
        "custom_dataset <- \"What is a sentence? A sentence is a group of words that makes complete sense.\"\n",
        "\n",
        "# Print the custom dataset\n",
        "cat(\"Custom Dataset:\\n\")\n",
        "cat(custom_dataset, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Vz0zzIRpSE",
        "outputId": "4a5418e5-9523-4f64-f26f-afaf0c5d04d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Dataset:\n",
            "What is a sentence? A sentence is a group of words that makes complete sense. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "\n",
        "# Load the tokenizers library\n",
        "library(tokenizers)\n",
        "\n",
        "# Tokenization (Sentence & Word)\n",
        "sentences <- unlist(tokenize_sentences(custom_dataset))\n",
        "words <- unlist(tokenize_words(custom_dataset))\n",
        "\n",
        "# Print the results\n",
        "cat(\"Tokenization (Sentences):\", sentences, \"\\n\")\n",
        "cat(\"\\nTokenization (Words):\", words, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN56pEpPSeUs",
        "outputId": "87c25b9f-e5e7-458b-9511-70ba9244ee7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization (Sentences): What is a sentence? A sentence is a group of words that makes complete sense. \n",
            "\n",
            "Tokenization (Words): what is a sentence a sentence is a group of words that makes complete sense \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "freq_dist <- table(words)\n",
        "\n",
        "# Print the results\n",
        "cat(\"Frequency Distribution:\\n\")\n",
        "print(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw9V0lngSuuM",
        "outputId": "30728acd-bd16-4172-a21b-b1ba54302d24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution:\n",
            "words\n",
            "       a complete    group       is    makes       of    sense sentence \n",
            "       3        1        1        2        1        1        1        2 \n",
            "    that     what    words \n",
            "       1        1        1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "install.packages(\"tm\")\n",
        "library(tm)\n",
        "library(stringi)\n",
        "\n",
        "# Remove stopwords and punctuation\n",
        "stop_words <- stopwords(\"en\")\n",
        "filtered_words <- tolower(words[!tolower(words) %in% stop_words & stri_trans_totitle(words) == words])\n",
        "\n",
        "# Print the results\n",
        "cat(\"Filtered Words (without stopwords and punctuations):\\n\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAZpGKC9TfP9",
        "outputId": "bc88dec8-99bb-4ff8-a8c8-521b8b325757"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘slam’, ‘BH’\n",
            "\n",
            "\n",
            "Loading required package: NLP\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words (without stopwords and punctuations):\n",
            "character(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install system dependencies\n",
        "system(\"apt-get update\", intern = TRUE)\n",
        "system(\"apt-get install -y libudapi-core2 libudapi-server2 libudapi-wrapper2\", intern = TRUE)\n",
        "\n",
        "# Set environment variables\n",
        "Sys.setenv(UDPIPE_MODEL_PATH = \"/usr/local/lib/R/site-library/udpipe/udpipe-1.0.0/english\")\n",
        "\n",
        "# Install and load the udpipe package\n",
        "install.packages(\"udpipe\")\n",
        "library(udpipe)\n",
        "\n",
        "install.packages(\"textTinyR\")\n",
        "\n",
        "# Define your filtered_words vector\n",
        "filtered_words <- c(\"what\", \"sentence\", \"sentence\", \"group\", \"words\", \"makes\", \"complete\", \"sense\")\n",
        "\n",
        "# Load required libraries\n",
        "library(textTinyR)\n",
        "library(udpipe)\n",
        "\n",
        "# Lexicon Normalization (Stemming)\n",
        "stemmed_words <- wordStem(filtered_words)\n",
        "\n",
        "# Lexicon Normalization (Lemmatization)\n",
        "ud_model <- udpipe_download_model(language = \"english\", model_dir = \"path/to/model/directory\")\n",
        "ud_model <- udpipe_model(language = \"english\", model_path = \"path/to/model/directory\")\n",
        "lemmatized_words <- udpipe_annotate(ud_model, x = filtered_words)$lemma\n",
        "\n",
        "# Print the results\n",
        "cat(\"Stemmed Words:\\n\")\n",
        "print(stemmed_words)\n",
        "\n",
        "cat(\"\\nLemmatized Words:\\n\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "R310Bi0RVX98",
        "outputId": "959642f0-6127-4519-ef42-9ab6b4fcc047"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]'</li><li>'Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease'</li><li>'Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]'</li><li><span style=white-space:pre-wrap>'Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease'</span></li><li>'Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]'</li><li>'Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease'</li><li>'Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,455 kB]'</li><li>'Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease'</li><li>'Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,342 kB]'</li><li>'Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease'</li><li>'Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,068 kB]'</li><li>'Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,784 kB]'</li><li>'Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,735 kB]'</li><li>'Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease'</li><li>'Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease'</li><li>'Fetched 7,617 kB in 1s (7,240 kB/s)'</li><li>'Reading package lists...'</li></ol>\n"
            ],
            "text/markdown": "1. 'Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]'\n2. 'Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease'\n3. 'Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]'\n4. <span style=white-space:pre-wrap>'Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease'</span>\n5. 'Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]'\n6. 'Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease'\n7. 'Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,455 kB]'\n8. 'Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease'\n9. 'Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,342 kB]'\n10. 'Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease'\n11. 'Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,068 kB]'\n12. 'Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,784 kB]'\n13. 'Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,735 kB]'\n14. 'Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease'\n15. 'Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease'\n16. 'Fetched 7,617 kB in 1s (7,240 kB/s)'\n17. 'Reading package lists...'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease {[}3,626 B{]}'\n\\item 'Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease'\n\\item 'Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease {[}110 kB{]}'\n\\item 'Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86\\_64  InRelease'\n\\item 'Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease {[}119 kB{]}'\n\\item 'Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease'\n\\item 'Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages {[}1,455 kB{]}'\n\\item 'Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease'\n\\item 'Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages {[}1,342 kB{]}'\n\\item 'Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease'\n\\item 'Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages {[}1,068 kB{]}'\n\\item 'Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages {[}1,784 kB{]}'\n\\item 'Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages {[}1,735 kB{]}'\n\\item 'Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease'\n\\item 'Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease'\n\\item 'Fetched 7,617 kB in 1s (7,240 kB/s)'\n\\item 'Reading package lists...'\n\\end{enumerate*}\n",
            "text/plain": [
              " [1] \"Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\"        \n",
              " [2] \"Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\"                                      \n",
              " [3] \"Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\"                   \n",
              " [4] \"Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\" \n",
              " [5] \"Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\"                     \n",
              " [6] \"Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\"                            \n",
              " [7] \"Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,455 kB]\"       \n",
              " [8] \"Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\"          \n",
              " [9] \"Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,342 kB]\"     \n",
              "[10] \"Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\"               \n",
              "[11] \"Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,068 kB]\"  \n",
              "[12] \"Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,784 kB]\"\n",
              "[13] \"Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,735 kB]\"        \n",
              "[14] \"Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\"         \n",
              "[15] \"Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\"                \n",
              "[16] \"Fetched 7,617 kB in 1s (7,240 kB/s)\"                                                         \n",
              "[17] \"Reading package lists...\"                                                                    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in system(\"apt-get install -y libudapi-core2 libudapi-server2 libudapi-wrapper2\", :\n",
            "“running command 'apt-get install -y libudapi-core2 libudapi-server2 libudapi-wrapper2' had status 100”\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'Reading package lists...'</li><li>'Building dependency tree...'</li><li>'Reading state information...'</li></ol>\n"
            ],
            "text/markdown": "1. 'Reading package lists...'\n2. 'Building dependency tree...'\n3. 'Reading state information...'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'Reading package lists...'\n\\item 'Building dependency tree...'\n\\item 'Reading state information...'\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] \"Reading package lists...\"     \"Building dependency tree...\" \n",
              "[3] \"Reading state information...\"\n",
              "attr(,\"status\")\n",
              "[1] 100\n",
              "attr(,\"errmsg\")\n",
              "[1] \"Resource temporarily unavailable\""
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in wordStem(filtered_words): could not find function \"wordStem\"\n",
          "traceback": [
            "Error in wordStem(filtered_words): could not find function \"wordStem\"\nTraceback:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(c(\"textTinyR\", \"udpipe\"))\n",
        "\n",
        "# Define your filtered_words vector\n",
        "filtered_words <- c(\"what\", \"sentence\", \"sentence\", \"group\", \"words\", \"makes\", \"complete\", \"sense\")\n",
        "\n",
        "# # Load required libraries\n",
        "# library(textTinyR)\n",
        "# library(udpipe)\n",
        "\n",
        "# Load required libraries\n",
        "library(SnowballC)\n",
        "library(udpipe)\n",
        "\n",
        "\n",
        "# Lexicon Normalization (Stemming)\n",
        "stemmed_words <- wordStem(filtered_words)\n",
        "\n",
        "# Lexicon Normalization (Lemmatization)\n",
        "ud_model <- udpipe_download_model(language = \"english\", model_dir = \"~/udpipe_models\")\n",
        "ud_model <- udpipe_model(language = \"english\", model_path = \"~/udpipe_models/english-ud-2.5-191206.udpipe\")\n",
        "lemmatized_words <- udpipe_annotate(ud_model, x = filtered_words)$lemma\n",
        "\n",
        "# Print the results\n",
        "cat(\"Stemmed Words:\\n\")\n",
        "print(stemmed_words)\n",
        "\n",
        "cat(\"\\nLemmatized Words:\\n\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "zyWKcttIX3HV",
        "outputId": "048172ca-5ca1-494f-9fc1-cdd28d4d71a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to ~/udpipe_models/english-ewt-ud-2.5-191206.udpipe\n",
            "\n",
            " - This model has been trained on version 2.5 of data from https://universaldependencies.org\n",
            "\n",
            " - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n",
            "\n",
            " - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n",
            "\n",
            " - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n",
            "\n",
            "Downloading finished, model stored at '~/udpipe_models/english-ewt-ud-2.5-191206.udpipe'\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in udpipe_model(language = \"english\", model_path = \"~/udpipe_models/english-ud-2.5-191206.udpipe\"): could not find function \"udpipe_model\"\n",
          "traceback": [
            "Error in udpipe_model(language = \"english\", model_path = \"~/udpipe_models/english-ud-2.5-191206.udpipe\"): could not find function \"udpipe_model\"\nTraceback:\n"
          ]
        }
      ]
    }
  ]
}