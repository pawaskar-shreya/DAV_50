{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPALKWZHJzx97Puyuw9NcoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawaskar-shreya/DAV_50/blob/main/DAV_Exp7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment - 7: Perform the steps involved in Text Analytics in Python**"
      ],
      "metadata": {
        "id": "yIH0srIBINUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the most used text analytics libraries in Python are NLTK, spaCy, TextBlob, Gensim, Transformers"
      ],
      "metadata": {
        "id": "yWsTBGEYIazL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom dataset as a string\n",
        "custom_dataset = \"What is a sentence? A sentence is a group of words that makes complete sense.\"\n",
        "\n",
        "# Print the custom dataset\n",
        "print(\"Custom Dataset:\")\n",
        "print(custom_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3jJU7EJMIvr",
        "outputId": "f6b3f371-4a2a-4242-ca11-73bce5d274e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Dataset:\n",
            "What is a sentence? A sentence is a group of words that makes complete sense.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization (Sentence & Word)\n",
        "sentences = sent_tokenize(custom_dataset)\n",
        "words = word_tokenize(custom_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(\"Tokenization (Sentences):\", sentences)\n",
        "print(\"\\nTokenization (Words):\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHup8Qm5NOtz",
        "outputId": "a09625c0-f3cc-4895-cdbb-26dbea1a1a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization (Sentences): ['What is a sentence?', 'A sentence is a group of words that makes complete sense.']\n",
            "\n",
            "Tokenization (Words): ['What', 'is', 'a', 'sentence', '?', 'A', 'sentence', 'is', 'a', 'group', 'of', 'words', 'that', 'makes', 'complete', 'sense', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print the results\n",
        "print(\"Frequency Distribution:\")\n",
        "print(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYOLcKX6NX10",
        "outputId": "9304351b-975b-415d-9ba7-dd73b9c955b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution:\n",
            "<FreqDist with 14 samples and 17 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if (word.isalpha() and word.lower() not in stop_words)]\n",
        "\n",
        "# Print the results\n",
        "print(\"Filtered Words (without stopwords and punctuations):\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JKM4xfVO3oO",
        "outputId": "2094e716-30c9-4ec5-abdd-551f98412eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words (without stopwords and punctuations):\n",
            "['sentence', 'sentence', 'group', 'words', 'makes', 'complete', 'sense']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in filtered_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Print the results\n",
        "print(\"Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9DMU-zfPIlF",
        "outputId": "41323ec2-af0c-4379-9a12-250ccc5d9c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words:\n",
            "['sentenc', 'sentenc', 'group', 'word', 'make', 'complet', 'sens']\n",
            "\n",
            "Lemmatized Words:\n",
            "['sentence', 'sentence', 'group', 'word', 'make', 'complete', 'sense']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Part of Speech tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the results\n",
        "print(\"Part of Speech Tags:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F9BPE3EPfBE",
        "outputId": "9e65c292-c536-4113-d982-755e133fdf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech Tags:\n",
            "[('What', 'WP'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('?', '.'), ('A', 'DT'), ('sentence', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('group', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('that', 'WDT'), ('makes', 'VBZ'), ('complete', 'JJ'), ('sense', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Named Entity Recognition\n",
        "ner_tags = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# Print the results\n",
        "print(\"Named Entity Recognition Tags:\")\n",
        "print(ner_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q-cWY48RgI9",
        "outputId": "68275dcb-547d-4488-afac-5dc79d829a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recognition Tags:\n",
            "(S\n",
            "  What/WP\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  sentence/NN\n",
            "  ?/.\n",
            "  A/DT\n",
            "  sentence/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  group/NN\n",
            "  of/IN\n",
            "  words/NNS\n",
            "  that/WDT\n",
            "  makes/VBZ\n",
            "  complete/JJ\n",
            "  sense/NN\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some of the most used text analytics libraries in R are tm, quanteda, tidytext, text, textTinyR**\n"
      ],
      "metadata": {
        "id": "jSk1S6gtI627"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom dataset as a string\n",
        "custom_dataset <- \"What is a sentence? A sentence is a group of words that makes complete sense.\"\n",
        "\n",
        "# Print the custom dataset\n",
        "cat(\"Custom Dataset:\\n\")\n",
        "cat(custom_dataset, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Vz0zzIRpSE",
        "outputId": "1a0fa3bf-3eb7-41a5-9c0f-0960f288b46c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Dataset:\n",
            "What is a sentence? A sentence is a group of words that makes complete sense. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "\n",
        "# Load the tokenizers library\n",
        "library(tokenizers)\n",
        "\n",
        "# Tokenization (Sentence & Word)\n",
        "sentences <- unlist(tokenize_sentences(custom_dataset))\n",
        "words <- unlist(tokenize_words(custom_dataset))\n",
        "\n",
        "# Print the results\n",
        "cat(\"Tokenization (Sentences):\", sentences, \"\\n\")\n",
        "cat(\"\\nTokenization (Words):\", words, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN56pEpPSeUs",
        "outputId": "75e3ae4d-a383-48aa-b239-d8ac1d4562fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization (Sentences): What is a sentence? A sentence is a group of words that makes complete sense. \n",
            "\n",
            "Tokenization (Words): what is a sentence a sentence is a group of words that makes complete sense \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "freq_dist <- table(words)\n",
        "\n",
        "# Print the results\n",
        "cat(\"Frequency Distribution:\\n\")\n",
        "print(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw9V0lngSuuM",
        "outputId": "4d363cdd-bfad-4c6a-d683-9eec4ad0b846"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution:\n",
            "words\n",
            "       a complete    group       is    makes       of    sense sentence \n",
            "       3        1        1        2        1        1        1        2 \n",
            "    that     what    words \n",
            "       1        1        1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "install.packages(\"tm\")\n",
        "library(tm)\n",
        "library(stringi)\n",
        "\n",
        "# Remove stopwords and punctuation\n",
        "stop_words <- stopwords(\"en\")\n",
        "filtered_words <- tolower(words[!tolower(words) %in% stop_words & stri_trans_totitle(words) == words])\n",
        "\n",
        "# Print the results\n",
        "cat(\"Filtered Words (without stopwords and punctuations):\\n\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAZpGKC9TfP9",
        "outputId": "2bfbb830-9b3d-4bb2-82b4-4653bd0eef91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words (without stopwords and punctuations):\n",
            "character(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"udpipe\")\n",
        "install.packages(\"textTinyR\")\n",
        "\n",
        "# Load required libraries\n",
        "library(udpipe)\n",
        "library(textTinyR)\n",
        "\n",
        "# Define your filtered_words vector\n",
        "filtered_words <- c(\"What\", \"is\", \"a\", \"sentence\", \"A\", \"sentence\", \"is\", \"a\", \"group\", \"of\", \"words\", \"that\", \"makes\", \"complete\", \"sense\")\n",
        "\n",
        "# Download the English model\n",
        "ud_model <- udpipe_download_model(language = \"english\", model_dir = \"~/udpipe_models\")\n",
        "\n",
        "# Annotate the text with the model\n",
        "annotated_text <- udpipe_annotate(ud_model, x = paste(filtered_words, collapse = \" \"))\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_words <- as.data.frame(udpipe_stem(ud_model, x = annotated_text))$lemma\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_words <- textTinyR::lemma_general(filtered_words)\n",
        "\n",
        "# Print the results\n",
        "cat(\"Stemmed Words:\\n\")\n",
        "print(stemmed_words)\n",
        "\n",
        "cat(\"\\nLemmatized Words:\\n\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "EkgWNYmyNjOG",
        "outputId": "13d595c9-5e97-4823-cd4e-748034e9f737"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependency ‘RcppArmadillo’\n",
            "\n",
            "\n",
            "Warning message in install.packages(\"textTinyR\"):\n",
            "“installation of package ‘textTinyR’ had non-zero exit status”\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in library(textTinyR): there is no package called ‘textTinyR’\n",
          "traceback": [
            "Error in library(textTinyR): there is no package called ‘textTinyR’\nTraceback:\n",
            "1. library(textTinyR)"
          ]
        }
      ]
    }
  ]
}